//
// auto-generated by op2.py
//

// Copyright 2009, Andrew Corrigan, acorriga@gmu.edu
// This code is from the AIAA-2009-4001 paper

// Warwick extensions:
// - multigrid, per-edge computation, n-neighbours
// - residual calculation, solution validation

//
// sparse tiling headers
//

#ifdef SLOPE
#include "executor.h"
#include "inspector.h"
#ifdef MPI_ON
#include "op_lib_mpi.h"
#include "op_mpi_core.h"
#endif
#endif

#define TILE_SIZE 5000

#include <stdio.h>
#include <iostream>
#include <fstream>
#include <cmath>
#include <string>
#include <omp.h>
#include <sys/time.h>
#include <sstream>
#include <cstdlib>
#include <map>
#include <vector>
#include <limits>

#include "hdf5.h"

#ifdef PAPI
#include "papi_funcs.h"
#endif

// #define LOG_PROGRESS

// OP2:
#include  "op_lib_cpp.h"

#ifdef SLOPE
#define double_ALIGN 128
#define float_ALIGN 64
#define int_ALIGN 64
#ifdef VECTORIZE
#define SIMD_VEC 8
#define ALIGNED_double __attribute__((aligned(double_ALIGN)))
#define ALIGNED_float __attribute__((aligned(float_ALIGN)))
#define ALIGNED_int __attribute__((aligned(int_ALIGN)))
#ifdef __ICC
  #define DECLARE_PTR_ALIGNED(X, Y) __assume_aligned(X, Y)
#else
  #define DECLARE_PTR_ALIGNED(X, Y)
#endif
// Note: two includes below pulled from slope/ subfolder:
#include "compute_flux_edge_kernel_veckernel.h"
#include "unstructured_stream_kernel_veckernel.h"
#include "compute_bnd_node_flux_kernel_veckernel.h"
#else
#define ALIGNED_double
#define ALIGNED_float
#define ALIGNED_int
#define DECLARE_PTR_ALIGNED(X, Y)
#endif
#endif

//
// op_par_loop declarations
//
#ifdef OPENACC
#ifdef __cplusplus
extern "C" {
#endif
#endif

void op_par_loop_initialize_variables_kernel(char const *, op_set,
  op_arg );

void op_par_loop_zero_5d_array_kernel(char const *, op_set,
  op_arg );

void op_par_loop_zero_1d_array_kernel(char const *, op_set,
  op_arg );

void op_par_loop_calculate_cell_volumes(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_dampen_ewt(char const *, op_set,
  op_arg );

void op_par_loop_copy_double_kernel(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_calculate_dt_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_get_min_dt_kernel(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_compute_step_factor_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_compute_flux_edge_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_compute_flux_edge_kernel_instrumented(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg
  #ifdef VERIFY_OP2_TIMING
    , double* // compute time
    , double* // sync time
  #endif
  , long* // iterations
  #ifdef PAPI
    , long_long*, int, int
  #endif
);

void op_par_loop_compute_bnd_node_flux_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_time_step_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_unstructured_stream_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_unstructured_stream_kernel_instrumented(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg
  #ifdef VERIFY_OP2_TIMING
    , double* // compute time
    , double* // sync time
  #endif
  , long* // iterations
  #ifdef PAPI
    , long_long*, int, int
  #endif
);

void op_par_loop_residual_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_calc_rms_kernel(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_count_bad_vals(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_up_pre_kernel(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_up_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_up_post_kernel(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_down_v2_kernel_pre(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_down_v2_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_down_v2_kernel_post(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_down_kernel(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_identify_differences(char const *, op_set,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_count_non_zeros(char const *, op_set,
  op_arg,
  op_arg );
#ifdef OPENACC
#ifdef __cplusplus
}
#endif
#endif

#include "op_hdf5.h"

// MG-CFD base:
#include "const.h"
#include "structures.h"
#include "inlined_funcs.h"
#include "config.h"
#include "utils.h"
#include "io.h"
#include "timer.h"

// Global scalars:
double smoothing_coefficient = double(0.2f);
double ff_variable[NVAR];
double ff_flux_contribution_momentum_x[NDIM];
double ff_flux_contribution_momentum_y[NDIM];
double ff_flux_contribution_momentum_z[NDIM];
double ff_flux_contribution_density_energy[NDIM];
int mesh_name;
#include "global.h"
#ifdef PAPI
int num_events;
#endif
config conf;

// MG-CFD kernels:
#include "flux.h"
#include "mg.h"
#include "time_stepping_kernels.h"
#include "compute_node_area_kernel.h"
#include "validation.h"
#include "unstructured_stream.h"
#include "misc.h"

#ifdef SLOPE
#ifdef MPI_ON

int get_max_value(int* arr, int from, int to){
  int max = 0;  // assumption: max >= 0
  for(int i = from; i < to; i++){
    if(max < arr[i]){
      max = arr[i];
    }  
  }
  return max;
}

void calculate_max_values(op_set* sets, int set_count, op_map* maps, int map_count,
std::map<op_set, int>* to_set_to_core_max, std::map<op_set, int>* to_set_to_exec_max, std::map<op_set, int>* to_set_to_nonexec_max, int my_rank){

  std::map<op_set, std::vector<int>> to_set_to_map_index;
  std::map<op_set, std::vector<int>>::iterator it;
  op_set to_set;

  for(int i = 0; i < map_count; i ++){
    to_set = maps[i]->to;
    it = to_set_to_map_index.find(to_set);
    if(it != to_set_to_map_index.end()){
      std::vector<int>* map_ids = &it->second;
      map_ids->push_back(i);
    }
    else{
      std::vector<int> map_ids;
      map_ids.push_back(i);
      to_set_to_map_index.insert(std::pair<op_set, std::vector<int>>(to_set, map_ids));
    }
  }
  
  for (auto it = to_set_to_map_index.begin(); it != to_set_to_map_index.end(); ++it){
    //get core max values
    int core_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      core_max[i] = get_max_value(maps[it->second.at(i)]->map, 0, maps[it->second.at(i)]->from->core_size * maps[it->second.at(i)]->dim);
    }

    int core_max_of_max = core_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(core_max_of_max < core_max[i])
        core_max_of_max = core_max[i];
    }
    to_set_to_core_max->insert(std::pair<op_set, int>(it->first, core_max_of_max));

    //get exec max values
    int exec_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      exec_max[i] = get_max_value(maps[it->second.at(i)]->map, maps[it->second.at(i)]->from->core_size * maps[it->second.at(i)]->dim,
      (maps[it->second.at(i)]->from->size +  OP_import_exec_list[maps[it->second.at(i)]->from->index]->size) * maps[it->second.at(i)]->dim);
    }

    int exec_max_of_max = exec_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(exec_max_of_max < exec_max[i])
        exec_max_of_max = exec_max[i];
    }
    to_set_to_exec_max->insert(std::pair<op_set, int>(it->first, exec_max_of_max));

    //get nonexec max values
    int nonexec_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      nonexec_max[i] = get_max_value(maps[it->second.at(i)]->map, (maps[it->second.at(i)]->from->size +  OP_import_exec_list[maps[it->second.at(i)]->from->index]->size) * maps[it->second.at(i)]->dim,
      (maps[it->second.at(i)]->from->size +  OP_import_exec_list[maps[it->second.at(i)]->from->index]->size + 
      OP_import_nonexec_list[maps[it->second.at(i)]->from->index]->size) * maps[it->second.at(i)]->dim);
    }

    int nonexec_max_of_max = nonexec_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(nonexec_max_of_max < nonexec_max[i])
        nonexec_max_of_max = nonexec_max[i];
    }
    to_set_to_nonexec_max->insert(std::pair<op_set, int>(it->first, nonexec_max_of_max));
  } 
}

int get_core_size(op_set set, std::map<op_set, int>* to_set_to_core_max){

  std::map<op_set, int>::iterator it;
  it = to_set_to_core_max->find(set);

  if(it != to_set_to_core_max->end()){
    return it->second + 1;
  }
  else{
    return set->core_size;
  }
}

int get_exec_size(op_set set, std::map<op_set, int>* to_set_to_core_max, std::map<op_set, int>* to_set_to_exec_max){

  std::map<op_set, int>::iterator it_core, it_exec;
  it_core = to_set_to_core_max->find(set);

  if(it_core != to_set_to_core_max->end()){
    it_exec = to_set_to_exec_max->find(set);
    return ((it_exec->second - it_core->second) > 0) ? (it_exec->second - it_core->second) : 0;
  }
  else{
    return (set->size - set->core_size) + OP_import_exec_list[set->index]->size;
  }
}

int get_nonexec_size(op_set set, std::map<op_set, int>* to_set_to_exec_max, std::map<op_set, int>* to_set_to_nonexec_max){

  std::map<op_set, int>::iterator it_exec, it_nonexec;
  it_exec = to_set_to_exec_max->find(set);

  if(it_exec != to_set_to_exec_max->end()){
    it_nonexec = to_set_to_nonexec_max->find(set);
    return ((it_nonexec->second - it_exec->second) > 0) ? (it_nonexec->second - it_exec->second) : 0;
  }
  else{
    return OP_import_nonexec_list[set->index]->size;
  }
}
#endif
#endif


int main(int argc, char** argv)
{
    #ifdef NANCHECK
        feenableexcept(FE_ALL_EXCEPT & ~FE_INEXACT);
    #endif
    
    set_config_defaults();
    if (!parse_arguments(argc, argv)) {
        return 1;
    }
    if (strcmp(conf.input_file, "") == 0) {
        op_printf("ERROR: input_file not set\n");
        return 1;
    }

    const char* input_file_name = conf.input_file;
    const char* input_directory = conf.input_file_directory;
    if (strcmp(input_directory, "")!=0) {
        input_file_name = strdup((std::string(input_directory) + "/" + input_file_name).c_str());
    }

    int problem_size = 0;
    int levels = 0;
    int base_array_index = 1;
    std::string* layers = NULL;
    std::string* mg_connectivity_filename = NULL;
    read_input_dat(input_file_name, &problem_size, &levels, &base_array_index, &layers, &mg_connectivity_filename);
    if (strcmp(input_directory, "")!=0) {
        for (int l=0; l<levels; l++) {
            layers[l] = (std::string(input_directory) + "/" + layers[l]).c_str();
            if (l < (levels-1))
                mg_connectivity_filename[l] = (std::string(input_directory) + "/" + mg_connectivity_filename[l]).c_str();
        }
    }

    if (base_array_index >= 1 && base_array_index <= 9) {
      // Append 'base_array_index' to args:

      char** new_argv = (char**)malloc((argc+1)*sizeof(char*));
      for (int i=0; i<argc; i++) {
        new_argv[i] = (char*)malloc((strlen(argv[i])+1)*sizeof(char));
        strcpy(new_argv[i], argv[i]);
      }
      new_argv[argc] = (char*)malloc((strlen("OP_MAPS_BASE_INDEX=0")+1)*sizeof(char));
      sprintf(new_argv[argc], "OP_MAPS_BASE_INDEX=%d", base_array_index);
      argc++;

      argv = new_argv;
    }

    #ifdef LOG_PROGRESS
        // op_init(argc, argv, 7); // Report positive checks in op_plan_check
        // op_init(argc, argv, 4);
        op_init(argc, argv, 3); // Report execution of parallel loops
        // op_init(argc, argv, 2); // Info on plan construction
        // op_init(argc, argv, 1); // Error-checking
    #else
        op_init(argc, argv, 0);
    #endif

    // timer
    double cpu_t1, cpu_t2, wall_t1, wall_t2;

    #ifdef VERIFY_OP2_TIMING
        double flux_kernel_compute_times[levels];
        for (int i=0; i<levels; i++) {
            flux_kernel_compute_times[i] = 0.0;
        }
        double flux_kernel_sync_times[levels];
        for (int i=0; i<levels; i++) {
            flux_kernel_sync_times[i] = 0.0;
        }
    #endif
    long flux_kernel_iter_counts[levels];
    for (int i=0; i<levels; i++) {
        flux_kernel_iter_counts[i] = 0;
    }
    long unstructured_stream_kernel_iter_counts[levels];
    for (int i=0; i<levels; i++) {
        unstructured_stream_kernel_iter_counts[i] = 0;
    }
    #ifdef PAPI
        int num_events = 0;
        init_papi(&num_events);
        long_long flux_kernel_event_counts[levels*num_events];
        for (int i=0; i<(levels*num_events); i++) {
            flux_kernel_event_counts[i] = 0;
        }
        long_long ustream_kernel_event_counts[levels*num_events];
        for (int i=0; i<(levels*num_events); i++) {
            ustream_kernel_event_counts[i] = 0;
        }

        int event_set;
        int* events;
        load_papi_events(num_events, &event_set, &events);
    #endif

    // set far field conditions
    {
        const double angle_of_attack = double(PI / 180.0) * double(deg_angle_of_attack);

        ff_variable[VAR_DENSITY] = double(1.4);

        double ff_pressure = double(1.0);
        double ff_speed_of_sound = sqrt(GAMMA*ff_pressure / ff_variable[VAR_DENSITY]);
        double ff_speed = double(ff_mach)*ff_speed_of_sound;

        double3 ff_velocity;
        ff_velocity.x = ff_speed*double(cos((double)angle_of_attack));
        ff_velocity.y = ff_speed*double(sin((double)angle_of_attack));
        ff_velocity.z = 0.0;

        ff_variable[VAR_MOMENTUM+0] = ff_variable[VAR_DENSITY] * ff_velocity.x;
        ff_variable[VAR_MOMENTUM+1] = ff_variable[VAR_DENSITY] * ff_velocity.y;
        ff_variable[VAR_MOMENTUM+2] = ff_variable[VAR_DENSITY] * ff_velocity.z;

        ff_variable[VAR_DENSITY_ENERGY] = ff_variable[VAR_DENSITY]*(double(0.5)*(ff_speed*ff_speed))
                                        + (ff_pressure / double(GAMMA-1.0));

        double3 ff_momentum;
        ff_momentum.x = *(ff_variable+VAR_MOMENTUM+0);
        ff_momentum.y = *(ff_variable+VAR_MOMENTUM+1);
        ff_momentum.z = *(ff_variable+VAR_MOMENTUM+2);
        compute_flux_contribution(ff_variable[VAR_DENSITY], ff_momentum,
                                    ff_variable[VAR_DENSITY_ENERGY],
                                    ff_pressure, ff_velocity,
                                    ff_flux_contribution_momentum_x,
                                    ff_flux_contribution_momentum_y,
                                    ff_flux_contribution_momentum_z,
                                    ff_flux_contribution_density_energy);
    }


   

    #ifdef SLOPE
    int avg_tile_size = conf.tile_size; 
    double* coordinates;
    int nloops = 5;
    int seed_loop = 0;
    // sets
    set_t* sl_nodes[levels];
    set_t* sl_edges[levels]; 
    set_t* sl_bnd_nodes[levels];

    map_t* sl_edge_to_nodes[levels];
    map_t* sl_bnd_node_to_node[levels];

    desc_list compute_flux_desc[levels];
    desc_list compute_bflux_desc[levels];
    desc_list time_step_desc[levels];
    desc_list unstructured_stream_desc[levels];
    desc_list zero_5d_array_desc[levels];

    map_list mesh_maps[levels];

    inspector_t* insp[levels];
    executor_t* exec[levels];
    std::string sl_maps_edge_to_nodes[levels];
    std::string sl_maps_bnd_node_to_node[levels];

    #ifdef MPI_ON
    int set_size;
    op_arg args0[levels][5];
    op_arg args1[levels][4];
    op_arg args2[levels][4];
    op_arg args3[levels][5];
    op_arg args4[levels][4];

    op_set* sets[levels];
    op_map* maps[levels];

    for(int i = 0; i < levels; i++){
      sets[i] = new op_set[3];
      maps[i] = new op_map[2];
    }

    int sl_rank = 0;
    int comm_size = 0;
    MPI_Comm_rank(MPI_COMM_WORLD, &sl_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);

    #endif
    #endif

    // Set elements:
    op_set op_nodes[levels],
           op_edges[levels],
           op_bnd_nodes[levels];

    // Set mappings:
    op_map p_edge_to_nodes[levels],
           p_bnd_node_to_node[levels]
           ;

    // MG mapping:
    op_map* p_node_to_mg_node = NULL;
    op_map* p_edge_to_mg_nodes = NULL;
    if (levels > 1) {
        p_node_to_mg_node = alloc<op_map>(levels-1);
        p_edge_to_mg_nodes = alloc<op_map>(levels-1);
    }

    // Set data:
    op_dat p_edge_weights[levels],
           p_bnd_node_weights[levels],
           p_bnd_node_groups[levels],
           p_node_coords[levels];

    op_dat variables_correct[levels];

    // Temporary set data (ie, arrays that are populated by kernels)
    op_dat p_variables[levels], 
           p_old_variables[levels], 
           p_residuals[levels],
           p_residuals_prolonged[levels],
           p_residuals_prolonged_wsum[levels],
           p_volumes[levels],
           p_step_factors[levels],
           p_fluxes[levels];
    op_dat p_dummy_variables[levels],
           p_dummy_fluxes[levels];
    // op_dat p_dummy_fluxes[levels]; // strictly for unstructured_stream
    op_dat p_up_scratch[levels];

    // Setup OP2
    char* op_name = alloc<char>(100);
    #ifdef SLOPE
    char* sl_name = alloc<char>(100);
    #endif
    {
              op_decl_const2("smoothing_coefficient",1,"double",&smoothing_coefficient);
              op_decl_const2("ff_variable",5,"double",ff_variable);
              op_decl_const2("ff_flux_contribution_momentum_x",3,"double",ff_flux_contribution_momentum_x);
              op_decl_const2("ff_flux_contribution_momentum_y",3,"double",ff_flux_contribution_momentum_y);
              op_decl_const2("ff_flux_contribution_momentum_z",3,"double",ff_flux_contribution_momentum_z);
              op_decl_const2("ff_flux_contribution_density_energy",3,"double",ff_flux_contribution_density_energy);
              op_decl_const2("mesh_name",1,"int",&mesh_name);

        op_printf("-----------------------------------------------------\n");
        op_printf("Loading from HDF5 files ...\n");

        for (int i=0; i<levels; i++) {
            op_printf("Loading level %d / %d\n", i+1, levels);
            
            sprintf(op_name, "op_nodes_L%d", i);
            if (conf.legacy_mode) {
                op_nodes[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "node_coordinates.renumbered");
            } else {
                op_nodes[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "node_coordinates");
            }

            if (conf.legacy_mode) {
                sprintf(op_name, "op_edges_L%d", i);
                op_edges[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "edge-->node.renumbered");
            } else {
                sprintf(op_name, "op_edges_L%d", i);
                op_edges[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "edge-->node");
            }

            if (conf.legacy_mode) {
                sprintf(op_name, "op_bnd_nodes_L%d", i);
                op_bnd_nodes[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "bnd_node-->node.renumbered");
            } else {
                sprintf(op_name, "op_bnd_nodes_L%d", i);
                op_bnd_nodes[i] = op_decl_set_hdf5_infer_size(layers[i].c_str(), op_name, "bnd_node-->node");
            }

            if (conf.legacy_mode) {
                p_edge_to_nodes[i]          = op_decl_map_hdf5(op_edges[i],     op_nodes[i], 2, layers[i].c_str(), "edge-->node.renumbered");
                p_bnd_node_to_node[i]       = op_decl_map_hdf5(op_bnd_nodes[i], op_nodes[i], 1, layers[i].c_str(), "bnd_node-->node.renumbered");
            } else {
                p_edge_to_nodes[i]          = op_decl_map_hdf5(op_edges[i],     op_nodes[i], 2, layers[i].c_str(), "edge-->node");
                p_bnd_node_to_node[i]       = op_decl_map_hdf5(op_bnd_nodes[i], op_nodes[i], 1, layers[i].c_str(), "bnd_node-->node");
            }
            p_bnd_node_groups[i]       = op_decl_dat_hdf5(op_bnd_nodes[i], 1, "int", layers[i].c_str(), "bnd_node-->group");

            if (i > 0) {
                sprintf(op_name, "op_node-->mg_node_L%d", i);
                if (conf.legacy_mode) {
                    p_node_to_mg_node[i-1] = op_decl_map_hdf5(op_nodes[i-1], op_nodes[i], 1, layers[i-1].c_str(), "node-->mg_node.renumbered");
                } else {
                    p_node_to_mg_node[i-1] = op_decl_map_hdf5(op_nodes[i-1], op_nodes[i], 1, layers[i-1].c_str(), "node-->mg_node");
                }

                // TODO: Generate the mapping p_edge_to_mg_nodes, where 
                //         p_edge_to_mg_nodes[i][0] == p_node_to_mg_node[p_edge_to_nodes[i][0]]
                //       and
                //         p_edge_to_mg_nodes[i][1] == p_node_to_mg_node[p_edge_to_nodes[i][1]]
                //       It may be necessary to assume that the input mesh is so large that 
                //       it prevents generation of this mapping in a single compute node.
                p_edge_to_mg_nodes[i-1] = NULL;
            }

            sprintf(op_name, "p_volumes_L%d", i);
            if (conf.legacy_mode) {
                p_volumes[i] = op_decl_dat_hdf5(op_nodes[i], 1, "double", layers[i].c_str(), "areas");
                p_volumes[i]->name = copy_str(op_name);
            }

            if (conf.legacy_mode) {
                p_edge_weights[i] = op_decl_dat_hdf5(op_edges[i],         NDIM, "double", layers[i].c_str(), "edge_weights.recalculated");
            } else {
                p_edge_weights[i] = op_decl_dat_hdf5(op_edges[i],         NDIM, "double", layers[i].c_str(), "edge_weights");
            }
            p_bnd_node_weights[i] = op_decl_dat_hdf5(op_bnd_nodes[i], NDIM, "double", layers[i].c_str(), "bnd_node_weights");

            if (conf.legacy_mode) {
                p_node_coords[i] = op_decl_dat_hdf5(op_nodes[i], NDIM, "double", layers[i].c_str(), "node_coordinates.renumbered");
            } else {
                p_node_coords[i] = op_decl_dat_hdf5(op_nodes[i], NDIM, "double", layers[i].c_str(), "node_coordinates");
            }

            if (conf.validate_result) {
                std::string variables_solution_filepath(conf.input_file_directory);
                if (variables_solution_filepath.size() > 0) {
                    variables_solution_filepath += "/";
                }
                variables_solution_filepath += "solution.variables.L" + number_to_string(i);
                variables_solution_filepath += ".cycles=" + number_to_string(conf.num_cycles);
                variables_solution_filepath += ".h5";

                // op_printf("Checking access of file %s ...\n", variables_solution_filepath.c_str());
                if (access(variables_solution_filepath.c_str(), R_OK) != -1) {
                    std::string dataset_name("p_variables_result_L");
                    dataset_name += number_to_string(i);
                    variables_correct[i] = op_decl_dat_hdf5(op_nodes[i], NVAR, "double", variables_solution_filepath.c_str(), dataset_name.c_str());
                }
                else {
                    op_printf("Cannot find level %d solution file: %s\n", i, variables_solution_filepath.c_str());
                    variables_correct[i] = NULL;
                }
            } else {
                variables_correct[i] = NULL;
            }
        }
        
        op_printf("-----------------------------------------------------\n");
        op_printf("Partitioning ...\n");
        if (conf.partitioner == Partitioners::Parmetis) {
            if (conf.partitioner_method == PartitionerMethods::Geom) {
                op_partition("PARMETIS", "GEOM", op_nodes[0], OP_ID, p_node_coords[0]);
            }
            else if (conf.partitioner_method == PartitionerMethods::KWay) {
                op_partition("PARMETIS", "KWAY", op_nodes[0], p_edge_to_nodes[0], p_node_coords[0]);
            }
            else if (conf.partitioner_method == PartitionerMethods::GeomKWay) {
                op_partition("PARMETIS", "GEOMKWAY", op_nodes[0], p_edge_to_nodes[0], p_node_coords[0]);
            }
        }
        else if (conf.partitioner == Partitioners::Ptscotch) {
            if (conf.partitioner_method == PartitionerMethods::Geom) {
                op_partition("PTSCOTCH", "GEOM", op_nodes[0], OP_ID, p_node_coords[0]);
            }
            else if (conf.partitioner_method == PartitionerMethods::KWay) {
                op_partition("PTSCOTCH", "KWAY", op_nodes[0], p_edge_to_nodes[0], p_node_coords[0]);
            }
            else if (conf.partitioner_method == PartitionerMethods::GeomKWay) {
                op_partition("PTSCOTCH", "GEOMKWAY", op_nodes[0], p_edge_to_nodes[0], p_node_coords[0]);
            }
        }
        else if (conf.partitioner == Partitioners::Inertial) {
            op_partition("INERTIAL", "", op_nodes[0], OP_ID, p_node_coords[0]);
        }
        op_printf("PARTITIONING COMPLETE NEW\n");
        op_renumber(p_edge_to_nodes[0]);

        #ifdef SLOPE

        for(int i = 0; i < levels; i++){
          #ifdef MPI_ON

          sets[i][0] = op_nodes[i];
          sets[i][1] = op_edges[i];
          sets[i][2] = op_bnd_nodes[i];

          maps[i][0] = p_edge_to_nodes[i];
          maps[i][1] = p_bnd_node_to_node[i];

          std::map<op_set, int> to_set_to_core_max;
          std::map<op_set, int> to_set_to_exec_max;
          std::map<op_set, int> to_set_to_nonexec_max;

          calculate_max_values(sets[i], 3, maps[i], 2, &to_set_to_core_max, &to_set_to_exec_max, &to_set_to_nonexec_max, sl_rank);
         
          #endif

          #ifdef MPI_ON

          sprintf(sl_name, "sl_nodes_L%d", i);
          sl_nodes[i] = set(sl_name, get_core_size(op_nodes[i], &to_set_to_core_max) , get_exec_size(op_nodes[i], &to_set_to_core_max, &to_set_to_exec_max), 
            get_nonexec_size(op_nodes[i], &to_set_to_exec_max, &to_set_to_nonexec_max));
          
          sprintf(sl_name, "sl_edges_L%d", i);
          sl_edges[i] = set(sl_name, get_core_size(op_edges[i], &to_set_to_core_max) , get_exec_size(op_edges[i], &to_set_to_core_max, &to_set_to_exec_max), 
            get_nonexec_size(op_edges[i], &to_set_to_exec_max, &to_set_to_nonexec_max));

          sprintf(sl_name, "sl_bnd_nodes_L%d", i);
          sl_bnd_nodes[i] = set(sl_name, get_core_size(op_bnd_nodes[i], &to_set_to_core_max) , get_exec_size(op_bnd_nodes[i], &to_set_to_core_max, &to_set_to_exec_max), 
            get_nonexec_size(op_bnd_nodes[i], &to_set_to_exec_max, &to_set_to_nonexec_max));

          #else

          sprintf(sl_name, "sl_nodes_L%d", i);
          sl_nodes[i] = set(sl_name, op_nodes[i]->size);

          sprintf(sl_name, "sl_edges_L%d", i);    
          sl_edges[i] = set(sl_name, op_edges[i]->size);

          sprintf(sl_name, "sl_bnd_nodes_L%d", i);
          sl_bnd_nodes[i] = set(sl_name, op_bnd_nodes[i]->size);

          #endif

          sprintf(sl_name, "sl_edge_to_nodes_L%d", i);
          sl_maps_edge_to_nodes[i] = sl_name;
          sl_edge_to_nodes[i] = map(sl_name, sl_edges[i], sl_nodes[i], p_edge_to_nodes[i]->map, sl_edges[i]->size * 2);

          sprintf(sl_name, "sl_bnd_node_to_node_L%d", i);
          sl_maps_bnd_node_to_node[i] = sl_name;
          sl_bnd_node_to_node[i] = map(sl_name, sl_bnd_nodes[i], sl_nodes[i], p_bnd_node_to_node[i]->map, sl_bnd_nodes[i]->size * 1);

          
          // descriptors
          desc_list flux_desc ({desc(sl_edge_to_nodes[i], READ),
                                  desc(DIRECT, READ),
                                  desc(sl_edge_to_nodes[i], INC)});
          compute_flux_desc[i] = flux_desc;

          desc_list bflux_desc ({desc(DIRECT, READ),
                                  desc(sl_bnd_node_to_node[i], READ),
                                  desc(sl_bnd_node_to_node[i], INC)});
          compute_bflux_desc[i] = bflux_desc;

          desc_list time_desc ({desc(DIRECT, READ),
                                  desc(DIRECT, INC),
                                  desc(DIRECT, WRITE)});
          time_step_desc[i] = time_desc;

          desc_list unstructured_stream_descriptor ({desc(sl_edge_to_nodes[i], READ),
                                  desc(DIRECT, READ),
                                  desc(sl_edge_to_nodes[i], INC)});
          unstructured_stream_desc[i] = unstructured_stream_descriptor;

          desc_list zero_5d_desc ({desc(DIRECT, WRITE)});
          zero_5d_array_desc[i] = zero_5d_desc;

          #ifdef MPI_ON
          insp[i] = insp_init(avg_tile_size, OMP_MPI);
          // insp[i] = insp_init(avg_tile_size, ONLY_MPI);
          #else
          map_list m ({sl_edge_to_nodes[i]});
          mesh_maps[i] = m;
          insp[i] = insp_init(avg_tile_size, OMP, COL_DEFAULT, &mesh_maps[i]);
          #endif

          sprintf(sl_name, "compute_flux_L%d", i);
          insp_add_parloop (insp[i], sl_name, sl_edges[i], &compute_flux_desc[i]);

          sprintf(sl_name, "compute_bflux_L%d", i);
          insp_add_parloop (insp[i], sl_name, sl_bnd_nodes[i], &compute_bflux_desc[i]);

          sprintf(sl_name, "time_step_L%d", i);
          insp_add_parloop (insp[i], sl_name, sl_nodes[i],  &time_step_desc[i]);

          sprintf(sl_name, "unstructured_stream_L%d", i);
          insp_add_parloop(insp[i], sl_name, sl_edges[i], &unstructured_stream_desc[i]);

          sprintf(sl_name, "zero_5d_L%d", i);
          insp_add_parloop(insp[i], sl_name, sl_nodes[i], &zero_5d_array_desc[i]);

          seed_loop = 0;
          insp_run (insp[i], seed_loop);
          
          #ifdef MPI_ON
          for (int k = 0; k < comm_size; k++) {
            if (k == sl_rank) {
              insp_print (insp[i], LOW);
              // generate_vtk (insp, HIGH, vertices, mesh->coords, DIM2, rank);
            }
            MPI_Barrier(MPI_COMM_WORLD);
          }
          #else
          insp_print (insp[i], LOW);
          #endif
          
/*
          coordinates = (double*) malloc(NDIM * (op_nodes[i]->size) * sizeof(double));
          int size = 3 * sizeof(double);
          for (int n = 0; n < op_nodes[i]->size; n++) {
              coordinates[3*n] = *(double*)(p_node_coords[i]->data + size * n);
              coordinates[3*n+1] = *(double*)(p_node_coords[i]->data + size * n + sizeof(double));
              coordinates[3*n+2] = *(double*)(p_node_coords[i]->data + size * n + (2 * sizeof(double)));
          }
          generate_vtk (insp[i], LOW, sl_nodes[i], coordinates, DIM3);
          free(coordinates);
*/
          exec[i] = exec_init (insp[i]);
        }
        #endif
        
        for (int i=0; i<levels; i++) {
            sprintf(op_name, "p_variables_L%d", i);
            p_variables[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);
            sprintf(op_name, "p_old_variables_L%d", i);
            p_old_variables[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);
            sprintf(op_name, "p_residuals_L%d", i);
            p_residuals[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);

            if (!conf.legacy_mode) {
                // Need to calculate cell volumes:
                sprintf(op_name, "p_volumes_L%d", i);
                p_volumes[i] = op_decl_dat_temp_char(op_nodes[i], 1, "double", sizeof(double), op_name);
            }

            sprintf(op_name, "p_step_factors_L%d", i);
            p_step_factors[i] = op_decl_dat_temp_char(op_nodes[i], 1, "double", sizeof(double), op_name);

            sprintf(op_name, "p_fluxes_L%d", i);
            p_fluxes[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);
            if (conf.measure_mem_bound) {
                sprintf(op_name, "p_dummy_variables_L%d", i);
                p_dummy_variables[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);
                sprintf(op_name, "p_dummy_fluxes_L%d", i);
                p_dummy_fluxes[i] = op_decl_dat_temp_char(op_nodes[i], NVAR, "double", sizeof(double), op_name);
            }

            if (i > 0) {
                sprintf(op_name, "p_up_scratch_L%d", i);
                p_up_scratch[i] = op_decl_dat_temp_char(op_nodes[i], 1, "int", sizeof(double), op_name);
            } else {
                p_up_scratch[i] = NULL;
            }
        }
    }

    // Initialise variables:
    for (int i=0; i<levels; i++) {
        op_par_loop_initialize_variables_kernel("initialize_variables_kernel",op_nodes[i],
                    op_arg_dat(p_variables[i],-1,OP_ID,5,"double",OP_WRITE));
        op_par_loop_zero_5d_array_kernel("zero_5d_array_kernel",op_nodes[i],
                    op_arg_dat(p_fluxes[i],-1,OP_ID,5,"double",OP_WRITE));
        if (conf.measure_mem_bound) {
            op_par_loop_zero_5d_array_kernel("zero_5d_array_kernel",op_nodes[i],
                        op_arg_dat(p_dummy_variables[i],-1,OP_ID,5,"double",OP_WRITE));
            op_par_loop_zero_5d_array_kernel("zero_5d_array_kernel",op_nodes[i],
                        op_arg_dat(p_dummy_fluxes[i],-1,OP_ID,5,"double",OP_WRITE));
        }
        if (!conf.legacy_mode) {
            op_par_loop_zero_1d_array_kernel("zero_1d_array_kernel",op_nodes[i],
                        op_arg_dat(p_volumes[i],-1,OP_ID,1,"double",OP_WRITE));
            op_par_loop_calculate_cell_volumes("calculate_cell_volumes",op_edges[i],
                        op_arg_dat(p_node_coords[i],0,p_edge_to_nodes[i],3,"double",OP_READ),
                        op_arg_dat(p_node_coords[i],1,p_edge_to_nodes[i],3,"double",OP_READ),
                        op_arg_dat(p_edge_weights[i],-1,OP_ID,3,"double",OP_INC),
                        op_arg_dat(p_volumes[i],0,p_edge_to_nodes[i],1,"double",OP_INC),
                        op_arg_dat(p_volumes[i],1,p_edge_to_nodes[i],1,"double",OP_INC));
        }
    }
    // Fudge the weights to delay occurrence of negative densities in HDF5 meshes:
    for (int l=0; l<levels; l++) {
        op_par_loop_dampen_ewt("dampen_ewt",op_edges[l],
                    op_arg_dat(p_edge_weights[l],-1,OP_ID,3,"double",OP_INC));
        op_par_loop_dampen_ewt("dampen_ewt",op_bnd_nodes[l],
                    op_arg_dat(p_bnd_node_weights[l],-1,OP_ID,3,"double",OP_INC));
    }
    op_printf("-----------------------------------------------------\n");
    op_printf("Compute beginning\n");

    op_timers(&cpu_t1, &wall_t1);
    int level = 0;
    int mg_dir = MG_UP;
    int i = 0;
    double rms = 0.0;
    int bad_val_count = 0;
    double min_dt = std::numeric_limits<double>::max();
    while(i < conf.num_cycles)
    {
        #ifdef LOG_PROGRESS
            op_printf("Performing MG cycle %d / %d", i+1, conf.num_cycles);
        #else
            if (level==0)
            op_printf("Performing MG cycle %d / %d", i+1, conf.num_cycles);
        #endif

        op_par_loop_copy_double_kernel("copy_double_kernel",op_nodes[level],
                    op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_READ),
                    op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_WRITE));

        // for the first iteration we compute the time step
        op_par_loop_calculate_dt_kernel("calculate_dt_kernel",op_nodes[level],
                    op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_READ),
                    op_arg_dat(p_volumes[level],-1,OP_ID,1,"double",OP_READ),
                    op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_WRITE));
        min_dt = std::numeric_limits<double>::max();
        op_par_loop_get_min_dt_kernel("get_min_dt_kernel",op_nodes[level],
                    op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_READ),
                    op_arg_gbl(&min_dt,1,"double",OP_MIN));
        if (min_dt < 0.0f) {
          op_printf("Fatal error during 'step factor' calculation, min_dt = %.5e\n", min_dt);
          op_exit();
          return 1;
        }
        op_par_loop_compute_step_factor_kernel("compute_step_factor_kernel",op_nodes[level],
                    op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_READ),
                    op_arg_dat(p_volumes[level],-1,OP_ID,1,"double",OP_READ),
                    op_arg_gbl(&min_dt,1,"double",OP_READ),
                    op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_WRITE));
        #ifdef SLOPE

        #ifdef MPI_ON

        args0[level][0] = op_arg_dat(p_variables[level],0,p_edge_to_nodes[level],5,"double",OP_READ);
        args0[level][1] = op_arg_dat(p_variables[level],1,p_edge_to_nodes[level],5,"double",OP_READ);
        args0[level][2] = op_arg_dat(p_edge_weights[level],-1,OP_ID,3,"double",OP_READ);
        args0[level][3] = op_arg_dat(p_fluxes[level],0,p_edge_to_nodes[level],5,"double",OP_INC);
        args0[level][4] = op_arg_dat(p_fluxes[level],1,p_edge_to_nodes[level],5,"double",OP_INC);
        
        args1[level][0] = op_arg_dat(p_bnd_node_groups[level],-1,OP_ID,1,"int",OP_READ);
        args1[level][1] = op_arg_dat(p_bnd_node_weights[level],-1,OP_ID,3,"double",OP_READ);
        args1[level][2] = op_arg_dat(p_variables[level],0,p_bnd_node_to_node[level],5,"double",OP_READ);
        args1[level][3] = op_arg_dat(p_fluxes[level],0,p_bnd_node_to_node[level],5,"double",OP_INC);
        
        args2[level][0] = op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_READ);
        args2[level][1] = op_arg_dat(p_fluxes[level],-1,OP_ID,5,"double",OP_INC);
        args2[level][2] = op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_READ);
        args2[level][3] = op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_WRITE);

        if (conf.measure_mem_bound) {
          args3[level][0] = op_arg_dat(p_variables[level],0,p_edge_to_nodes[level],5,"double",OP_READ);
          args3[level][1] = op_arg_dat(p_variables[level],1,p_edge_to_nodes[level],5,"double",OP_READ);
          args3[level][2] = op_arg_dat(p_edge_weights[level],-1,OP_ID,3,"double",OP_READ);
          args3[level][3] = op_arg_dat(p_dummy_fluxes[level],0,p_edge_to_nodes[level],5,"double",OP_INC);
          args3[level][4] = op_arg_dat(p_dummy_fluxes[level],1,p_edge_to_nodes[level],5,"double",OP_INC);
          
          args4[level][0] = op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_READ);
          args4[level][1] = op_arg_dat(p_dummy_fluxes[level],-1,OP_ID,5,"double",OP_INC);
          args4[level][2] = op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_READ);
          args4[level][3] = op_arg_dat(p_dummy_variables[level],-1,OP_ID,5,"double",OP_WRITE);
        }

        #endif

        const int compute_flux_op2_id = 9;
        const int unstructured_stream_op2_id = 12;
        op_timing_realloc_manytime(compute_flux_op2_id, omp_get_max_threads());
        op_timing_realloc_manytime(unstructured_stream_op2_id, omp_get_max_threads());
        int ncolors = exec_num_colors (exec[level]);
        #ifdef PAPI
          // Init PAPI
          long_long* temp_count_stores = NULL;
          if (num_events > 0) {
            temp_count_stores = (long_long*)malloc(sizeof(long_long)*num_events);
            for (int e=0; e<num_events; e++) {
              temp_count_stores[e] = 0;
            }
          }
        #endif

        #endif
        int rkCycle;
        for (rkCycle=0; rkCycle<RK; rkCycle++)
        {
            #ifdef LOG_PROGRESS
                op_printf(" RK cycle %d / %d\n", rkCycle+1, RK);
            #endif

            #ifdef SLOPE
            //for each colour
            OP_kernels[compute_flux_op2_id].count++;
            OP_kernels[unstructured_stream_op2_id].count++;
            for (int color = 0; color < ncolors; color++) {
            // for all tiles of this color
                const int n_tiles_per_color = exec_tiles_per_color (exec[level], color);

                // #pragma omp parallel for
                // for (int j = 0; j < n_tiles_per_color; j++) {
                // Switch to manual OMP decomposition to enable thread timers:
                #pragma omp parallel
                {
                double thr_wall_t1, thr_wall_t2;
                thr_wall_t1 = omp_get_wtime();

                int nthreads = omp_get_num_threads();
                int thr = omp_get_thread_num();
                int thr_start = (n_tiles_per_color * thr) / nthreads;
                int thr_end = (n_tiles_per_color * (thr+1)) / nthreads;
                if (thr_end > n_tiles_per_color) thr_end = n_tiles_per_color;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_start(event_set);
                    }
                  }
                #endif

                for (int j=thr_start; j<thr_end; j++) {

                    // execute the tile
                    #ifdef MPI_ON
                    tile_t* tile = exec_tile_at (exec[level], color, j, LOCAL);
                    if(tile == NULL)
                      continue;
                    #else
                    tile_t* tile = exec_tile_at (exec[level], color, j);
                    #endif
                    int loop_size;

                    // loop compute_flux_edge
                    iterations_list& le2n_0 = tile_get_local_map (tile, 0, sl_maps_edge_to_nodes[level]);
                    iterations_list& iterations_0 = tile_get_iterations (tile, 0);
                    loop_size = tile_loop_size (tile, 0);
                    double* p_variables_data = (double*)(p_variables[level]->data);
                    const double* p_edge_weights_data = (double*)(p_edge_weights[level]->data);
                    double* p_fluxes_data    = (double*)(p_fluxes[level]->data);
                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          compute_flux_edge_kernel(
                              &p_variables_data[le2n_0[k*2 + 0] * 5],
                              &p_variables_data[le2n_0[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_0[k] * 3],
                              &p_fluxes_data[le2n_0[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_0[k*2 + 1] * 5]);
                      }
                    #else
                      int simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_double double dat0[5][SIMD_VEC];
                      ALIGNED_double double dat1[5][SIMD_VEC];
                      ALIGNED_double double dat2[3][SIMD_VEC];
                      ALIGNED_double double dat3[5][SIMD_VEC];
                      ALIGNED_double double dat4[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx0 = le2n_0[k*2 + 0];
                            int idx1 = le2n_0[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                dat0[v][sl] = p_variables_data[idx0*5 + v];
                                dat1[v][sl] = p_variables_data[idx1*5 + v];
                                dat3[v][sl] = 0.0;
                                dat4[v][sl] = 0.0;
                            }

                            int ewt_idx = iterations_0[k];
                            #pragma unroll
                            for (int v=0; v<3; v++) {
                              dat2[v][sl] = p_edge_weights_data[ewt_idx*3 + v];
                            }
                          }

                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            compute_flux_edge_kernel_vec(
                              dat0,
                              dat1,
                              dat2,
                              dat3,
                              dat4,
                              sl);
                          }

                          for ( int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx3 = le2n_0[k*2 + 0];
                            int idx4 = le2n_0[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                p_fluxes_data[idx3*5 + v] += dat3[v][sl];
                                p_fluxes_data[idx4*5 + v] += dat4[v][sl];
                            }
                          }
                      }

                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          compute_flux_edge_kernel(
                              &p_variables_data[le2n_0[k*2 + 0] * 5],
                              &p_variables_data[le2n_0[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_0[k] * 3],
                              &p_fluxes_data[le2n_0[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_0[k*2 + 1] * 5]);
                      }
                    #endif

                    // loop compute_bnd_node_flux
                    // iterations_list& lbe2n_1 = tile_get_local_map (tile, 1, sl_maps_bnd_node_to_node[level]);
                    // iterations_list& iterations_1 = tile_get_iterations (tile, 1);
                    // loop_size = tile_loop_size (tile, 1);
                  
                    // for (int k = 0; k < loop_size; k++) {
                    //     compute_bnd_node_flux_kernel(
                    //          (int*)(p_bnd_node_groups[level]->data + ((iterations_1[k] * 1) * sizeof(int))),
                    //          (double*)(p_bnd_node_weights[level]->data + ((iterations_1[k] * 3) * sizeof(double))),
                    //          (double*)(p_variables[level]->data + ((lbe2n_1[k + 0] * 5) * sizeof(double))),
                    //          (double*)(p_fluxes[level]->data + ((lbe2n_1[k + 0] * 5) * sizeof(double))));
                    // }

                    iterations_list& lbe2n_1 = tile_get_local_map (tile, 1, sl_maps_bnd_node_to_node[level]);
                    iterations_list& iterations_1 = tile_get_iterations (tile, 1);
                    loop_size = tile_loop_size (tile, 1);

                    const int* p_bnd_node_groups_data = (int*)(p_bnd_node_groups[level]->data);
                    const double* p_bnd_node_weights_data = (double*)(p_bnd_node_weights[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          compute_bnd_node_flux_kernel(
                              &p_bnd_node_groups_data[iterations_1[k] * 1],
                              &p_bnd_node_weights_data[iterations_1[k] * 3],
                              &p_variables_data[lbe2n_1[k + 0] * 5],
                              &p_fluxes_data[lbe2n_1[k + 0] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_int int dat10[1][SIMD_VEC];
                      ALIGNED_double double dat11[3][SIMD_VEC];
                      ALIGNED_double double dat12[5][SIMD_VEC];
                      ALIGNED_double double dat13[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_1[k];
                          int idx1 = iterations_1[k];
                          int idx2 = lbe2n_1[k + 0];
                            

                          dat10[0][sl] = p_bnd_node_groups_data[idx0*1];

                          #pragma unroll
                          for (int v=0; v<3; v++) {
                              dat11[v][sl] = p_bnd_node_weights_data[idx1*3 + v];
                          }

                          #pragma unroll
                          for (int v=0; v<5; v++) {
                              dat12[v][sl] = p_variables_data[idx2*5 + v];
                              dat13[v][sl] = 0.0;
                          }
                        }
                  
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          compute_bnd_node_flux_kernel_vec(
                            dat10,
                            dat11,
                            dat12,
                            dat13,
                            sl);
                        }

                        for ( int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx3 = lbe2n_1[k + 0];

                          #pragma unroll
                          for (int v=0; v<5; v++) {
                              p_fluxes_data[idx3*5 + v] += dat13[v][sl];
                          }
                        }
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          compute_bnd_node_flux_kernel(
                              &p_bnd_node_groups_data[iterations_1[k] * 1],
                              &p_bnd_node_weights_data[iterations_1[k] * 3],
                              &p_variables_data[lbe2n_1[k + 0] * 5],
                              &p_fluxes_data[lbe2n_1[k + 0] * 5]);
                      }
                    
                    #endif

                    // loop time_step
                    // iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    // loop_size = tile_loop_size (tile, 2);
                    
                    // for (int k = 0; k < loop_size; k++) {
                    //     time_step_kernel(
                    //         &rkCycle,
                    //         (double*)(p_step_factors[level]->data + ((iterations_2[k] * 1) * sizeof(double))),
                    //         (double*)(p_fluxes[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_old_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))));
                    // }

                    iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    loop_size = tile_loop_size (tile, 2);

                    const double* p_step_factors_data = (double*)(p_step_factors[level]->data);
                    const double* p_old_variables_data = (double*)(p_old_variables[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_variables_data[iterations_2[k] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_2[k];
                          time_step_kernel(
                             &rkCycle,
                              &p_step_factors_data[idx0],
                              &p_fluxes_data[idx0 * 5],
                              &p_old_variables_data[idx0 * 5],
                              &p_variables_data[idx0 * 5]);
                        }                        
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_variables_data[iterations_2[k] * 5]);
                      }
                    
                    #endif

                }
                thr_wall_t2 = omp_get_wtime();
                OP_kernels[compute_flux_op2_id].name = "fluxes_and_timestep";
                OP_kernels[compute_flux_op2_id].times[thr]  += thr_wall_t2 - thr_wall_t1;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_stop(&flux_kernel_event_counts[level*num_events], temp_count_stores, event_set, num_events);
                      for (int e=0; e<num_events; e++) temp_count_stores[e] = 0;
                    }
                  }
                #endif

                } // Close omp parallel
            }

            if (conf.measure_mem_bound)
            {
            // Execute tiled unstructured_stream() + time_step(). These kernels are inherently 
            // data-bound, their performance will establish performance-bound of above 
            // tiled compute_flux_edge() + time_step()
            //for each colour
            for (int color = 0; color < ncolors; color++) {
            // for all tiles of this color
                const int n_tiles_per_color = exec_tiles_per_color (exec[level], color);

                // #pragma omp parallel for
                // for (int j = 0; j < n_tiles_per_color; j++) {
                // Switch to manual OMP decomposition to enable thread timers:
                #pragma omp parallel
                {
                double thr_wall_t1, thr_wall_t2;
                    thr_wall_t1 = omp_get_wtime();

                int nthreads = omp_get_num_threads();
                int thr = omp_get_thread_num();
                int thr_start = (n_tiles_per_color * thr) / nthreads;
                int thr_end = (n_tiles_per_color * (thr+1)) / nthreads;
                if (thr_end > n_tiles_per_color) thr_end = n_tiles_per_color;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_start(event_set);
                    }
                  }
                #endif

                for (int j=thr_start; j<thr_end; j++) {
                  
                    // execute the tile
                    #ifdef MPI_ON
                    tile_t* tile = exec_tile_at (exec[level], color, j, LOCAL);
                    if(tile == NULL)
                      continue;
                    #else
                    tile_t* tile = exec_tile_at (exec[level], color, j);
                    #endif
                    int loop_size;

                    // loop unstructured_stream
                    iterations_list& le2n_3 = tile_get_local_map (tile, 3, sl_maps_edge_to_nodes[level]);
                    iterations_list& iterations_3 = tile_get_iterations (tile, 3);
                    loop_size = tile_loop_size (tile, 3);
                    double* p_variables_data = (double*)(p_dummy_variables[level]->data);
                    const double* p_edge_weights_data = (double*)(p_edge_weights[level]->data);
                    double* p_fluxes_data    = (double*)(p_dummy_fluxes[level]->data);
                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          unstructured_stream_kernel(
                                &p_variables_data[le2n_3[k*2 + 0] * 5],
                                &p_variables_data[le2n_3[k*2 + 1] * 5],
                                &p_edge_weights_data[iterations_3[k] * 3],
                                &p_fluxes_data[le2n_3[k*2 + 0] * 5],
                                &p_fluxes_data[le2n_3[k*2 + 1] * 5]);
                      }
                    #else

                      int simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_double double dat0[5][SIMD_VEC];
                      ALIGNED_double double dat1[5][SIMD_VEC];
                      ALIGNED_double double dat2[3][SIMD_VEC];
                      ALIGNED_double double dat3[5][SIMD_VEC];
                      ALIGNED_double double dat4[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx0 = le2n_3[k*2 + 0];
                            int idx1 = le2n_3[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                dat0[v][sl] = p_variables_data[idx0*5 + v];
                                dat1[v][sl] = p_variables_data[idx1*5 + v];
                                dat3[v][sl] = 0.0;
                                dat4[v][sl] = 0.0;
                            }

                            int ewt_idx = iterations_3[k];
                            #pragma unroll
                            for (int v=0; v<3; v++) {
                                dat2[v][sl] = p_edge_weights_data[ewt_idx*3 + v];
                            }
                          }

                          #ifdef __clang__
                              #pragma clang loop vectorize_width(SIMD_VEC)
                              #pragma nounroll
                          #else
                              #pragma omp simd simdlen(SIMD_VEC)
                          #endif
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            unstructured_stream_kernel_vec(
                              dat0,
                              dat1,
                              dat2,
                              dat3,
                              dat4,
                              sl);
                          }

                          for ( int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx3 = le2n_3[k*2 + 0];
                            int idx4 = le2n_3[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                p_fluxes_data[idx3*5 + v] += dat3[v][sl];
                                p_fluxes_data[idx4*5 + v] += dat4[v][sl];
                            }
                          }
                      }

                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          unstructured_stream_kernel(
                              &p_variables_data[le2n_3[k*2 + 0] * 5],
                              &p_variables_data[le2n_3[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_3[k] * 3],
                              &p_fluxes_data[le2n_3[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_3[k*2 + 1] * 5]);
                      }
                    #endif

                    // time_step
                    // iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    // loop_size = tile_loop_size (tile, 2);

                    // for (int k = 0; k < loop_size; k++) {
                    //     time_step_kernel(
                    //         &rkCycle,
                    //         (double*)(p_step_factors[level]->data + ((iterations_2[k] * 1) * sizeof(double))),
                    //         (double*)(p_dummy_fluxes[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_old_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_dummy_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))));
                    // }

                    iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    loop_size = tile_loop_size (tile, 2);

                    const double* p_step_factors_data = (double*)(p_step_factors[level]->data);
                    double* p_dummy_fluxes_data = (double*)(p_dummy_fluxes[level]->data);
                    const double* p_old_variables_data = (double*)(p_old_variables[level]->data);
                    double* p_dummy_variables_data = (double*)(p_dummy_variables[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_dummy_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_dummy_variables_data[iterations_2[k] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_2[k];
                          time_step_kernel(
                             &rkCycle,
                              &p_step_factors_data[idx0],
                              &p_dummy_fluxes_data[idx0 * 5],
                              &p_old_variables_data[idx0 * 5],
                              &p_dummy_variables_data[idx0 * 5]);
                        }                        
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_dummy_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_dummy_variables_data[iterations_2[k] * 5]);
                      }
                    
                    #endif

                }
                thr_wall_t2 = omp_get_wtime();
                OP_kernels[unstructured_stream_op2_id].name = "fluxesRW_and_timestep";
                OP_kernels[unstructured_stream_op2_id].times[thr]  += thr_wall_t2 - thr_wall_t1;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_stop(&unstructured_stream_kernel_event_counts[level*num_events], temp_count_stores, event_set, num_events);
                      for (int e=0; e<num_events; e++) temp_count_stores[e] = 0;
                    }
                  }
                #endif

                } // Close omp parallel

            }
            }

            //start exec halo execution
            #ifdef MPI_ON
            op_mpi_set_dirtybit(5, args0[level]);
            op_mpi_set_dirtybit(4, args1[level]);
            op_mpi_set_dirtybit(4, args2[level]);
            op_mpi_set_dirtybit(5, args3[level]);
            op_mpi_set_dirtybit(4, args3[level]);
      
            set_size = op_mpi_halo_exchanges(op_edges[level], 5, args0[level]);
            set_size = op_mpi_halo_exchanges(op_bnd_nodes[level], 4, args1[level]);
            set_size = op_mpi_halo_exchanges(op_nodes[level], 4, args2[level]);
            set_size = op_mpi_halo_exchanges(op_edges[level], 5, args3[level]);
            set_size = op_mpi_halo_exchanges(op_nodes[level], 4, args3[level]);

            op_mpi_wait_all(5, args0[level]);
            op_mpi_wait_all(4, args1[level]);
            op_mpi_wait_all(4, args2[level]);
            op_mpi_wait_all(5, args3[level]);
            op_mpi_wait_all(4, args3[level]);

            for (int color = 0; color < ncolors; color++) {
            // for all tiles of this color
                const int n_tiles_per_color = exec_tiles_per_color (exec[level], color);

                // #pragma omp parallel for
                // for (int j = 0; j < n_tiles_per_color; j++) {
                // Switch to manual OMP decomposition to enable thread timers:
                #pragma omp parallel
                {
                double thr_wall_t1, thr_wall_t2;
                thr_wall_t1 = omp_get_wtime();

                int nthreads = omp_get_num_threads();
                int thr = omp_get_thread_num();
                int thr_start = (n_tiles_per_color * thr) / nthreads;
                int thr_end = (n_tiles_per_color * (thr+1)) / nthreads;
                if (thr_end > n_tiles_per_color) thr_end = n_tiles_per_color;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_start(event_set);
                    }
                  }
                #endif

                for (int j=thr_start; j<thr_end; j++) {

                    // execute the tile
                    tile_t* tile = exec_tile_at (exec[level], color, j, EXEC_HALO);
                    if(tile == NULL)
                      continue;
                    int loop_size;

                    // loop compute_flux_edge
                    iterations_list& le2n_0 = tile_get_local_map (tile, 0, sl_maps_edge_to_nodes[level]);
                    iterations_list& iterations_0 = tile_get_iterations (tile, 0);
                    loop_size = tile_loop_size (tile, 0);
                    double* p_variables_data = (double*)(p_variables[level]->data);
                    const double* p_edge_weights_data = (double*)(p_edge_weights[level]->data);
                    double* p_fluxes_data    = (double*)(p_fluxes[level]->data);
                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          compute_flux_edge_kernel(
                              &p_variables_data[le2n_0[k*2 + 0] * 5],
                              &p_variables_data[le2n_0[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_0[k] * 3],
                              &p_fluxes_data[le2n_0[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_0[k*2 + 1] * 5]);
                      }
                    #else
                      int simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_double double dat0[5][SIMD_VEC];
                      ALIGNED_double double dat1[5][SIMD_VEC];
                      ALIGNED_double double dat2[3][SIMD_VEC];
                      ALIGNED_double double dat3[5][SIMD_VEC];
                      ALIGNED_double double dat4[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx0 = le2n_0[k*2 + 0];
                            int idx1 = le2n_0[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                dat0[v][sl] = p_variables_data[idx0*5 + v];
                                dat1[v][sl] = p_variables_data[idx1*5 + v];
                                dat3[v][sl] = 0.0;
                                dat4[v][sl] = 0.0;
                            }

                            int ewt_idx = iterations_0[k];
                            #pragma unroll
                            for (int v=0; v<3; v++) {
                              dat2[v][sl] = p_edge_weights_data[ewt_idx*3 + v];
                            }
                          }

                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            compute_flux_edge_kernel_vec(
                              dat0,
                              dat1,
                              dat2,
                              dat3,
                              dat4,
                              sl);
                          }

                          for ( int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx3 = le2n_0[k*2 + 0];
                            int idx4 = le2n_0[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                p_fluxes_data[idx3*5 + v] += dat3[v][sl];
                                p_fluxes_data[idx4*5 + v] += dat4[v][sl];
                            }
                          }
                      }

                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          compute_flux_edge_kernel(
                              &p_variables_data[le2n_0[k*2 + 0] * 5],
                              &p_variables_data[le2n_0[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_0[k] * 3],
                              &p_fluxes_data[le2n_0[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_0[k*2 + 1] * 5]);
                      }
                    #endif

                    // loop compute_bnd_node_flux
                    // iterations_list& lbe2n_1 = tile_get_local_map (tile, 1, sl_maps_bnd_node_to_node[level]);
                    // iterations_list& iterations_1 = tile_get_iterations (tile, 1);
                    // loop_size = tile_loop_size (tile, 1);
                  
                    // for (int k = 0; k < loop_size; k++) {
                    //     compute_bnd_node_flux_kernel(
                    //          (int*)(p_bnd_node_groups[level]->data + ((iterations_1[k] * 1) * sizeof(int))),
                    //          (double*)(p_bnd_node_weights[level]->data + ((iterations_1[k] * 3) * sizeof(double))),
                    //          (double*)(p_variables[level]->data + ((lbe2n_1[k + 0] * 5) * sizeof(double))),
                    //          (double*)(p_fluxes[level]->data + ((lbe2n_1[k + 0] * 5) * sizeof(double))));
                    // }

                    iterations_list& lbe2n_1 = tile_get_local_map (tile, 1, sl_maps_bnd_node_to_node[level]);
                    iterations_list& iterations_1 = tile_get_iterations (tile, 1);
                    loop_size = tile_loop_size (tile, 1);

                    const int* p_bnd_node_groups_data = (int*)(p_bnd_node_groups[level]->data);
                    const double* p_bnd_node_weights_data = (double*)(p_bnd_node_weights[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          compute_bnd_node_flux_kernel(
                              &p_bnd_node_groups_data[iterations_1[k] * 1],
                              &p_bnd_node_weights_data[iterations_1[k] * 3],
                              &p_variables_data[lbe2n_1[k + 0] * 5],
                              &p_fluxes_data[lbe2n_1[k + 0] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_int int dat10[1][SIMD_VEC];
                      ALIGNED_double double dat11[3][SIMD_VEC];
                      ALIGNED_double double dat12[5][SIMD_VEC];
                      ALIGNED_double double dat13[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_1[k];
                          int idx1 = iterations_1[k];
                          int idx2 = lbe2n_1[k + 0];
                            

                          dat10[0][sl] = p_bnd_node_groups_data[idx0*1];

                          #pragma unroll
                          for (int v=0; v<3; v++) {
                              dat11[v][sl] = p_bnd_node_weights_data[idx1*3 + v];
                          }

                          #pragma unroll
                          for (int v=0; v<5; v++) {
                              dat12[v][sl] = p_variables_data[idx2*5 + v];
                              dat13[v][sl] = 0.0;
                          }
                        }
                  
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          compute_bnd_node_flux_kernel_vec(
                            dat10,
                            dat11,
                            dat12,
                            dat13,
                            sl);
                        }

                        for ( int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx3 = lbe2n_1[k + 0];

                          #pragma unroll
                          for (int v=0; v<5; v++) {
                              p_fluxes_data[idx3*5 + v] += dat13[v][sl];
                          }
                        }
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          compute_bnd_node_flux_kernel(
                              &p_bnd_node_groups_data[iterations_1[k] * 1],
                              &p_bnd_node_weights_data[iterations_1[k] * 3],
                              &p_variables_data[lbe2n_1[k + 0] * 5],
                              &p_fluxes_data[lbe2n_1[k + 0] * 5]);
                      }
                    
                    #endif

                    // loop time_step
                    // iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    // loop_size = tile_loop_size (tile, 2);
                    
                    // for (int k = 0; k < loop_size; k++) {
                    //     time_step_kernel(
                    //         &rkCycle,
                    //         (double*)(p_step_factors[level]->data + ((iterations_2[k] * 1) * sizeof(double))),
                    //         (double*)(p_fluxes[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_old_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))));
                    // }

                    iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    loop_size = tile_loop_size (tile, 2);

                    const double* p_step_factors_data = (double*)(p_step_factors[level]->data);
                    const double* p_old_variables_data = (double*)(p_old_variables[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_variables_data[iterations_2[k] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_2[k];
                          time_step_kernel(
                             &rkCycle,
                              &p_step_factors_data[idx0],
                              &p_fluxes_data[idx0 * 5],
                              &p_old_variables_data[idx0 * 5],
                              &p_variables_data[idx0 * 5]);
                        }                        
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_variables_data[iterations_2[k] * 5]);
                      }
                    
                    #endif

                }
                thr_wall_t2 = omp_get_wtime();
                OP_kernels[compute_flux_op2_id].name = "fluxes_and_timestep";
                OP_kernels[compute_flux_op2_id].times[thr]  += thr_wall_t2 - thr_wall_t1;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_stop(&flux_kernel_event_counts[level*num_events], temp_count_stores, event_set, num_events);
                      for (int e=0; e<num_events; e++) temp_count_stores[e] = 0;
                    }
                  }
                #endif

                } // Close omp parallel
            }

            if (conf.measure_mem_bound)
            {
            // Execute tiled unstructured_stream() + time_step(). These kernels are inherently 
            // data-bound, their performance will establish performance-bound of above 
            // tiled compute_flux_edge() + time_step()
            //for each colour
            for (int color = 0; color < ncolors; color++) {
            // for all tiles of this color
                const int n_tiles_per_color = exec_tiles_per_color (exec[level], color);

                // #pragma omp parallel for
                // for (int j = 0; j < n_tiles_per_color; j++) {
                // Switch to manual OMP decomposition to enable thread timers:
                #pragma omp parallel
                {
                double thr_wall_t1, thr_wall_t2;
                    thr_wall_t1 = omp_get_wtime();

                int nthreads = omp_get_num_threads();
                int thr = omp_get_thread_num();
                int thr_start = (n_tiles_per_color * thr) / nthreads;
                int thr_end = (n_tiles_per_color * (thr+1)) / nthreads;
                if (thr_end > n_tiles_per_color) thr_end = n_tiles_per_color;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_start(event_set);
                    }
                  }
                #endif

                for (int j=thr_start; j<thr_end; j++) {
                  
                    // execute the tile
                    tile_t* tile = exec_tile_at (exec[level], color, j, EXEC_HALO);
                    if(tile == NULL)
                      continue;
                   
                    int loop_size;

                    // loop unstructured_stream
                    iterations_list& le2n_3 = tile_get_local_map (tile, 3, sl_maps_edge_to_nodes[level]);
                    iterations_list& iterations_3 = tile_get_iterations (tile, 3);
                    loop_size = tile_loop_size (tile, 3);
                    double* p_variables_data = (double*)(p_dummy_variables[level]->data);
                    const double* p_edge_weights_data = (double*)(p_edge_weights[level]->data);
                    double* p_fluxes_data    = (double*)(p_dummy_fluxes[level]->data);
                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          unstructured_stream_kernel(
                                &p_variables_data[le2n_3[k*2 + 0] * 5],
                                &p_variables_data[le2n_3[k*2 + 1] * 5],
                                &p_edge_weights_data[iterations_3[k] * 3],
                                &p_fluxes_data[le2n_3[k*2 + 0] * 5],
                                &p_fluxes_data[le2n_3[k*2 + 1] * 5]);
                      }
                    #else

                      int simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      ALIGNED_double double dat0[5][SIMD_VEC];
                      ALIGNED_double double dat1[5][SIMD_VEC];
                      ALIGNED_double double dat2[3][SIMD_VEC];
                      ALIGNED_double double dat3[5][SIMD_VEC];
                      ALIGNED_double double dat4[5][SIMD_VEC];

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                          #pragma omp simd simdlen(SIMD_VEC)
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx0 = le2n_3[k*2 + 0];
                            int idx1 = le2n_3[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                dat0[v][sl] = p_variables_data[idx0*5 + v];
                                dat1[v][sl] = p_variables_data[idx1*5 + v];
                                dat3[v][sl] = 0.0;
                                dat4[v][sl] = 0.0;
                            }

                            int ewt_idx = iterations_3[k];
                            #pragma unroll
                            for (int v=0; v<3; v++) {
                                dat2[v][sl] = p_edge_weights_data[ewt_idx*3 + v];
                            }
                          }

                          #ifdef __clang__
                              #pragma clang loop vectorize_width(SIMD_VEC)
                              #pragma nounroll
                          #else
                              #pragma omp simd simdlen(SIMD_VEC)
                          #endif
                          for (int sl=0; sl<SIMD_VEC; sl++ ){
                            unstructured_stream_kernel_vec(
                              dat0,
                              dat1,
                              dat2,
                              dat3,
                              dat4,
                              sl);
                          }

                          for ( int sl=0; sl<SIMD_VEC; sl++ ){
                            int k = n+sl;
                            int idx3 = le2n_3[k*2 + 0];
                            int idx4 = le2n_3[k*2 + 1];

                            #pragma unroll
                            for (int v=0; v<5; v++) {
                                p_fluxes_data[idx3*5 + v] += dat3[v][sl];
                                p_fluxes_data[idx4*5 + v] += dat4[v][sl];
                            }
                          }
                      }

                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          unstructured_stream_kernel(
                              &p_variables_data[le2n_3[k*2 + 0] * 5],
                              &p_variables_data[le2n_3[k*2 + 1] * 5],
                              &p_edge_weights_data[iterations_3[k] * 3],
                              &p_fluxes_data[le2n_3[k*2 + 0] * 5],
                              &p_fluxes_data[le2n_3[k*2 + 1] * 5]);
                      }
                    #endif

                    // time_step
                    // iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    // loop_size = tile_loop_size (tile, 2);

                    // for (int k = 0; k < loop_size; k++) {
                    //     time_step_kernel(
                    //         &rkCycle,
                    //         (double*)(p_step_factors[level]->data + ((iterations_2[k] * 1) * sizeof(double))),
                    //         (double*)(p_dummy_fluxes[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_old_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))),
                    //         (double*)(p_dummy_variables[level]->data + ((iterations_2[k] * 5) * sizeof(double))));
                    // }

                    iterations_list& iterations_2 = tile_get_iterations (tile, 2);
                    loop_size = tile_loop_size (tile, 2);

                    const double* p_step_factors_data = (double*)(p_step_factors[level]->data);
                    double* p_dummy_fluxes_data = (double*)(p_dummy_fluxes[level]->data);
                    const double* p_old_variables_data = (double*)(p_old_variables[level]->data);
                    double* p_dummy_variables_data = (double*)(p_dummy_variables[level]->data);

                    #ifndef VECTORIZE
                      for (int k = 0; k < loop_size; k++) {
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_dummy_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_dummy_variables_data[iterations_2[k] * 5]);
                      }
                    #else
                      simd_end = (loop_size/SIMD_VEC)*SIMD_VEC;

                      for (int n=0 ; n < simd_end; n+=SIMD_VEC) {
                          // "sl" is SIMD lane:
                        #pragma omp simd simdlen(SIMD_VEC)
                        for (int sl=0; sl<SIMD_VEC; sl++ ){
                          int k = n+sl;
                          int idx0 = iterations_2[k];
                          time_step_kernel(
                             &rkCycle,
                              &p_step_factors_data[idx0],
                              &p_dummy_fluxes_data[idx0 * 5],
                              &p_old_variables_data[idx0 * 5],
                              &p_dummy_variables_data[idx0 * 5]);
                        }                        
                      }
                      
                      // remainder:
                      for (int n = simd_end ; n < loop_size; n++) {
                          int k = n;
                          time_step_kernel(
                              &rkCycle,
                              &p_step_factors_data[iterations_2[k] * 1],
                              &p_dummy_fluxes_data[iterations_2[k] * 5],
                              &p_old_variables_data[iterations_2[k] * 5],
                              &p_dummy_variables_data[iterations_2[k] * 5]);
                      }
                    
                    #endif

                }
                thr_wall_t2 = omp_get_wtime();
                OP_kernels[unstructured_stream_op2_id].name = "fluxesRW_and_timestep";
                OP_kernels[unstructured_stream_op2_id].times[thr]  += thr_wall_t2 - thr_wall_t1;

                #ifdef PAPI
                  if (thr == 0) {
                    if (num_events > 0) {
                      my_papi_stop(&unstructured_stream_kernel_event_counts[level*num_events], temp_count_stores, event_set, num_events);
                      for (int e=0; e<num_events; e++) temp_count_stores[e] = 0;
                    }
                  }
                #endif

                } // Close omp parallel

            }
            }

            #endif
	
            #else

            op_par_loop_compute_flux_edge_kernel_instrumented("compute_flux_edge_kernel",op_edges[level],
                        op_arg_dat(p_variables[level],0,p_edge_to_nodes[level],5,"double",OP_READ),
                        op_arg_dat(p_variables[level],1,p_edge_to_nodes[level],5,"double",OP_READ),
                        op_arg_dat(p_edge_weights[level],-1,OP_ID,3,"double",OP_READ),
                        op_arg_dat(p_fluxes[level],0,p_edge_to_nodes[level],5,"double",OP_INC),
                        op_arg_dat(p_fluxes[level],1,p_edge_to_nodes[level],5,"double",OP_INC)
                        #ifdef VERIFY_OP2_TIMING
                          , &flux_kernel_compute_times[level], &flux_kernel_sync_times[level]
                        #endif
                        , &flux_kernel_iter_counts[level]
                        #ifdef PAPI
                        , &flux_kernel_event_counts[level*num_events], event_set, num_events
                        #endif
                        );

            op_par_loop_compute_bnd_node_flux_kernel("compute_bnd_node_flux_kernel",op_bnd_nodes[level],
                        op_arg_dat(p_bnd_node_groups[level],-1,OP_ID,1,"int",OP_READ),
                        op_arg_dat(p_bnd_node_weights[level],-1,OP_ID,3,"double",OP_READ),
                        op_arg_dat(p_variables[level],0,p_bnd_node_to_node[level],5,"double",OP_READ),
                        op_arg_dat(p_fluxes[level],0,p_bnd_node_to_node[level],5,"double",OP_INC));

            op_par_loop_time_step_kernel("time_step_kernel",op_nodes[level],
                        op_arg_gbl(&rkCycle,1,"int",OP_READ),
                        op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_READ),
                        op_arg_dat(p_fluxes[level],-1,OP_ID,5,"double",OP_INC),
                        op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_READ),
                        op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_WRITE));

            if (conf.measure_mem_bound) {
                op_par_loop_unstructured_stream_kernel_instrumented("unstructured_stream_kernel",op_edges[level],
                            op_arg_dat(p_variables[level],0,p_edge_to_nodes[level],5,"double",OP_READ),
                            op_arg_dat(p_variables[level],1,p_edge_to_nodes[level],5,"double",OP_READ),
                            op_arg_dat(p_edge_weights[level],-1,OP_ID,3,"double",OP_READ),
                            op_arg_dat(p_dummy_fluxes[level],0,p_edge_to_nodes[level],5,"double",OP_INC),
                            op_arg_dat(p_dummy_fluxes[level],1,p_edge_to_nodes[level],5,"double",OP_INC)
                            #ifdef VERIFY_OP2_TIMING
                              , &unstructured_stream_kernel_compute_times[level], &unstructured_stream_kernel_sync_times[level]
                            #endif
                            , &unstructured_stream_kernel_iter_counts[level]
                            #ifdef PAPI
                            , &ustream_kernel_event_counts[level*num_events], event_set, num_events
                            #endif
                            );

                // Repeating time_step() is not necessary for measuring non-SLOPE memory-bound, only necessary 
                // to equalise call count (because it IS necessary measuring for SLOPE memory-bound):
                op_par_loop_time_step_kernel("time_step_kernel",op_nodes[level],
                            op_arg_gbl(&rkCycle,1,"int",OP_READ),
                            op_arg_dat(p_step_factors[level],-1,OP_ID,1,"double",OP_READ),
                            op_arg_dat(p_dummy_fluxes[level],-1,OP_ID,5,"double",OP_INC),
                            op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_READ),
                            op_arg_dat(p_dummy_variables[level],-1,OP_ID,5,"double",OP_WRITE));
            }

            #endif
        }

        op_par_loop_residual_kernel("residual_kernel",op_nodes[level],
                    op_arg_dat(p_old_variables[level],-1,OP_ID,5,"double",OP_READ),
                    op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_READ),
                    op_arg_dat(p_residuals[level],-1,OP_ID,5,"double",OP_WRITE));
        if (level == 0) {
            rms = 0.0;
            op_par_loop_calc_rms_kernel("calc_rms_kernel",op_nodes[level],
                        op_arg_dat(p_residuals[level],-1,OP_ID,5,"double",OP_READ),
                        op_arg_gbl(&rms,1,"double",OP_INC));
            rms = sqrt(rms / double(op_get_size(op_nodes[level])));
            // op_printf(" (RMS = %.3e)", rms);
            // Until I get the HDF5 meshes working correctly, no point displaying incorrect RMS.

            #ifdef OPENACC
              // count_bad_vals() invokes isnan(), unsupported with OpenACC.
            #else
                op_par_loop_count_bad_vals("count_bad_vals",op_nodes[level],
                            op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_READ),
                            op_arg_gbl(&bad_val_count,1,"int",OP_INC));
            #endif
            if (bad_val_count > 0) {
                op_printf("Bad variable values detected, aborting\n");
                op_exit();
                return 1;
            }
            op_printf("\n");
        }

        if (levels <= 1) {
            i++;
        }
        else {
            if(mg_dir == MG_UP)
            {
                level++;

                op_par_loop_up_pre_kernel("up_pre_kernel",op_nodes[level-1],
                            op_arg_dat(p_variables[level],0,p_node_to_mg_node[level-1],5,"double",OP_WRITE),
                            op_arg_dat(p_up_scratch[level],0,p_node_to_mg_node[level-1],1,"int",OP_WRITE));

                op_par_loop_up_kernel("up_kernel",op_nodes[level-1],
                            op_arg_dat(p_variables[level-1],-1,OP_ID,5,"double",OP_READ),
                            op_arg_dat(p_variables[level],0,p_node_to_mg_node[level-1],5,"double",OP_INC),
                            op_arg_dat(p_up_scratch[level],0,p_node_to_mg_node[level-1],1,"int",OP_INC));

                op_par_loop_up_post_kernel("up_post_kernel",op_nodes[level],
                            op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_INC),
                            op_arg_dat(p_up_scratch[level],-1,OP_ID,1,"int",OP_READ));

                if(level == levels-1)
                {
                    mg_dir = MG_DOWN;
                }
            }
            else
            {
                level--;

                if (p_edge_to_mg_nodes[level] != NULL) {
                    // NOTE: Because I have not yet generated the mapping 'p_edge_to_mg_nodes', I have not 
                    // tested these 'down_v2_kernel' loops at all. I have simply ported it from MG-CFD-app-plain
                    op_par_loop_down_v2_kernel_pre("down_v2_kernel_pre",op_nodes[level],
                                op_arg_dat(p_residuals_prolonged[level],-1,OP_ID,5,"double",OP_WRITE),
                                op_arg_dat(p_residuals_prolonged_wsum[level],-1,OP_ID,1,"double",OP_WRITE));
                    op_par_loop_down_v2_kernel("down_v2_kernel",op_edges[level],
                                op_arg_dat(p_node_coords[level],0,p_edge_to_nodes[level],3,"double",OP_READ),
                                op_arg_dat(p_node_coords[level],1,p_edge_to_nodes[level],3,"double",OP_READ),
                                op_arg_dat(p_node_coords[level+1],0,p_edge_to_mg_nodes[level],3,"double",OP_READ),
                                op_arg_dat(p_node_coords[level+1],1,p_edge_to_mg_nodes[level],3,"double",OP_READ),
                                op_arg_dat(p_residuals[level+1],0,p_edge_to_mg_nodes[level],5,"double",OP_READ),
                                op_arg_dat(p_residuals[level+1],1,p_edge_to_mg_nodes[level],5,"double",OP_READ),
                                op_arg_dat(p_residuals_prolonged[level],0,p_edge_to_nodes[level],5,"double",OP_INC),
                                op_arg_dat(p_residuals_prolonged[level],1,p_edge_to_nodes[level],5,"double",OP_INC),
                                op_arg_dat(p_residuals_prolonged_wsum[level],0,p_edge_to_nodes[level],1,"double",OP_INC),
                                op_arg_dat(p_residuals_prolonged_wsum[level],1,p_edge_to_nodes[level],1,"double",OP_INC));
                    op_par_loop_down_v2_kernel_post("down_v2_kernel_post",op_nodes[level],
                                op_arg_dat(p_residuals_prolonged[level],-1,OP_ID,5,"double",OP_READ),
                                op_arg_dat(p_residuals_prolonged_wsum[level],-1,OP_ID,1,"double",OP_READ),
                                op_arg_dat(p_residuals[level],-1,OP_ID,5,"double",OP_READ),
                                op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_INC));
                } else {
                    op_par_loop_down_kernel("down_kernel",op_nodes[level],
                                op_arg_dat(p_variables[level],-1,OP_ID,5,"double",OP_INC),
                                op_arg_dat(p_residuals[level],-1,OP_ID,5,"double",OP_READ),
                                op_arg_dat(p_node_coords[level],-1,OP_ID,3,"double",OP_READ),
                                op_arg_dat(p_residuals[level+1],0,p_node_to_mg_node[level],5,"double",OP_READ),
                                op_arg_dat(p_node_coords[level+1],0,p_node_to_mg_node[level],3,"double",OP_READ));
                }

                if(level == 0)
                {
                    mg_dir = MG_UP;
                    i++;
                }
            }
        }
    }
    op_printf("\n");
	op_printf("Compute complete\n");

    op_timers(&cpu_t2, &wall_t2);
    op_printf("Max total runtime = %f\n", wall_t2 - wall_t1);

    // Write summary performance data to stdout:
    op_timing_output();

    // Write full performance data to file:
    std::string csv_out_filepath(conf.output_file_prefix);
    csv_out_filepath += "op2_performance_data.csv";
    op_printf("Writing OP2 timings to file: %s\n", csv_out_filepath.c_str());
    op_timings_to_csv(csv_out_filepath.c_str());

    if (conf.validate_result) {
        op_printf("-----------------------------------------------------\n");

        bool value_check_failed = false;
        op_printf("Looking for NaN and infinity values ...");
        for (int l=0; l<levels; l++) {
            int bad_val_count = 0;
            op_par_loop_count_bad_vals("count_bad_vals",op_nodes[l],
                        op_arg_dat(p_variables[l],-1,OP_ID,5,"double",OP_READ),
                        op_arg_gbl(&bad_val_count,1,"int",OP_INC));
            if (bad_val_count > 0) {
                value_check_failed = true;
                op_printf("\n");
                op_printf("Value check of MG level %d failed: %d bad values detected\n", l, bad_val_count);
                break;
            }
        }
        if (!value_check_failed) {
            op_printf(" None found\n");
            bool validation_failed = false;
            op_printf("Validating result against solution ...");
            for (int l=0; l<levels; l++) {
                if (variables_correct[l] == NULL) {
                    op_printf("\n");
                    op_printf("- Do not have solution for level %d, cannot validate\n", l);
                    validation_failed = true;
                    continue;
                }

                sprintf(op_name, "p_var_diff_L%d", l);
                op_dat variables_difference = op_decl_dat_temp_char(op_nodes[l], NVAR, "double", sizeof(double), op_name);

                op_par_loop_identify_differences("identify_differences",op_nodes[l],
                            op_arg_dat(p_variables[l],-1,OP_ID,5,"double",OP_READ),
                            op_arg_dat(variables_correct[l],-1,OP_ID,5,"double",OP_READ),
                            op_arg_dat(variables_difference,-1,OP_ID,5,"double",OP_WRITE));

                int count = 0;
                op_par_loop_count_non_zeros("count_non_zeros",op_nodes[l],
                            op_arg_dat(variables_difference,-1,OP_ID,5,"double",OP_READ),
                            op_arg_gbl(&count,1,"int",OP_INC));
                // Tolerate a tiny number of differences (as false positives):
                int threshold = op_get_size(op_nodes[l]) / 5000;
                if (count > threshold) {
                    validation_failed = true;
                    op_printf("\n");
                    op_printf("Validation of MG level %d failed: %d incorrect values in 'variables' array\n", l, count);
                    break;
                } else {
                    // op_printf("Validation of MG level %d successful\n", l);
                }
            }

            if (validation_failed) {
                op_printf("Validation failed\n");
            } else {
                op_printf(" Result correct\n");
                op_printf("Validation passed\n");
            }
        }
    }

    if (conf.output_anything) {
        op_printf("-----------------------------------------------------\n");
        op_printf("Writing out data...\n");
        char* h5_out_name = alloc<char>(100);
        std::string prefix(conf.output_file_prefix);
        for (int l=0; l<levels; l++)
        {
            std::string suffix = std::string(".L") + number_to_string(l) 
                               + "." + "cycles=" + number_to_string(conf.num_cycles);

            int number_of_edges = op_get_size(op_edges[l]);
            int nel     = op_get_size(op_nodes[l]);

            // Dump volumes:
            if (conf.output_volumes) {
                const char* old_name = p_volumes[l]->name;
                sprintf(op_name, "p_volumes_result_L%d", l);
                p_volumes[l]->name = strdup(op_name);
                sprintf(h5_out_name, "%svolumes%s.h5", prefix.c_str(), suffix.c_str());
                op_fetch_data_hdf5_file(p_volumes[l], h5_out_name);
                p_volumes[l]->name = old_name;
            }

            // Dump step factors:
            if (conf.output_step_factors) {
                const char* old_name = p_step_factors[l]->name;
                sprintf(op_name, "p_step_factors_result_L%d", l);
                p_step_factors[l]->name = strdup(op_name);
                sprintf(h5_out_name, "%sstep_factors%s.h5", prefix.c_str(), suffix.c_str());
                op_fetch_data_hdf5_file(p_step_factors[l], h5_out_name);
                p_step_factors[l]->name = old_name;
            }

            // Dump fluxes:
            if (conf.output_fluxes) {
                const char* old_name = p_fluxes[l]->name;
                sprintf(op_name, "p_fluxes_result_L%d", l);
                p_fluxes[l]->name = strdup(op_name);
                sprintf(h5_out_name, "%sfluxes%s.h5", prefix.c_str(), suffix.c_str());
                op_fetch_data_hdf5_file(p_fluxes[l], h5_out_name);
                p_fluxes[l]->name = old_name;
            }
            
            // Dump variables:
            if (conf.output_variables) {
                const char* old_name = p_variables[l]->name;
                sprintf(op_name, "p_variables_result_L%d", l);
                p_variables[l]->name = strdup(op_name);
                sprintf(h5_out_name, "%svariables%s.h5", prefix.c_str(), suffix.c_str());
                op_fetch_data_hdf5_file(p_variables[l], h5_out_name);
                p_variables[l]->name = old_name;
            }
        }
    }

    int my_rank=0;
    #ifdef MPI_ON
    op_rank(&my_rank);
    #endif
    #ifdef PAPI
        dump_papi_counters_to_file(
            my_rank, 
            levels, 
            num_events, 
            events, 
            flux_kernel_event_counts, 
            unstructured_stream_kernel_event_counts,
            conf.output_file_prefix);
    #endif

    #ifdef DUMP_EXT_PERF_DATA
        dump_perf_data_to_file(
            my_rank, 
            levels, 
            #ifdef VERIFY_OP2_TIMING
                flux_kernel_compute_times, 
                flux_kernel_sync_times,
            #endif
            flux_kernel_iter_counts, 
            conf.output_file_prefix);
    #endif

    op_print_dat_to_txtfile(p_variables[0], "pvariables_mpi.dat"); // ASCI

    op_printf("-----------------------------------------------------\n");
    op_printf("Winding down OP2\n");
    op_exit();

    return 0;
}
